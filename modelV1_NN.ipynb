{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# First, we need to remove all of the unesseary features, since i picked all beach breaks on the pacific west coast, we are treating\n",
    "# this as one surf break, no need to include directios relative to the shore line nor location/date/times.\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'waves_fileV4b.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define secondary swell columns to check\n",
    "secondary_swell_cols = [\n",
    "    \"Seconday 1 -> height\", \"Seconday 1 -> period\", \"Seconday 1 -> letters\", \"Seconday 1 -> angle\", \"Seconday 1\n",
    "    \"Seconday 2 -> height\", \"Seconday 2 -> period\", \"Seconday 2 -> letters\", \"Seconday 2 -> angle\",\n",
    "    \"Seconday 3 -> height\", \"Seconday 3 -> period\", \"Seconday 3 -> letters\", \"Seconday 3 -> angle\"\n",
    "]\n",
    "\n",
    "# Iterate through rows and check for missing secondary swell values\n",
    "for index, row in data.iterrows():\n",
    "    for col in secondary_swell_cols:\n",
    "        if pd.isna(row[col]) or row[col] == '':\n",
    "            data.at[index, col] = None  # Replace missing or empty values with None\n",
    "\n",
    "# Dropping Columns that are not nessesary for predicitng surf quality\n",
    "\n",
    "# List of columns to drop\n",
    "columns_to_drop = [\n",
    "    \"DocumentID\", \"DatesDocumentID\", \"timestamp\", \"name\", \"County\", \"Name\", \"data\",\n",
    "    \"Surf Height: Direction\", \"Surf Height: Angle\", \n",
    "    \"Seconday 1 -> letters\", \"Seconday 1 -> angle\", \n",
    "    \"Primary -> letters\", \"Primary -> angle\", \n",
    "    \"Seconday 2 -> letters\", \"Seconday 2 -> angle\", \n",
    "    \"Seconday 3 -> letters\", \"Seconday 3 -> angle\", \n",
    "    \"Wind Direction (NSEW)\", \"Wind Direction (Angle)\"\n",
    "]\n",
    "\n",
    "# Drop the columns from the DataFrame\n",
    "data_swell = data.drop(columns=columns_to_drop)\n",
    "\n",
    "# shuffling all lines, forecasts are cumlative which means it would be biased if we split the data without shuffling\n",
    "data_swell = data_swell.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "data = data_swell\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(columns=['Star Rating'])  # Features\n",
    "y = data['Star Rating']                # Target\n",
    "\n",
    "X = pd.get_dummies(X, columns=[\"Wind Direction (Human Relation)\"], drop_first=True)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 14:39:45.362186: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Define the neural network\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[0;32m----> 7\u001b[0m     Dense(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[43mX_train_scaled\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],)),  \u001b[38;5;66;03m# Input layer\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     Dense(\u001b[38;5;241m32\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),                                          \u001b[38;5;66;03m# Hidden layer\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     Dense(\u001b[38;5;241m1\u001b[39m)                                                               \u001b[38;5;66;03m# Output layer\u001b[39;00m\n\u001b[1;32m     10\u001b[0m ])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define the neural network\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),  # Input layer\n",
    "    Dense(32, activation='relu'),                                          # Hidden layer\n",
    "    Dense(1)                                                               # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_test_scaled, y_test),\n",
    "    epochs=50,       # Adjust as needed\n",
    "    batch_size=32    # Adjust as needed\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
